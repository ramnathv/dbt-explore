{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Boilerplate for a dbt Project\n",
    "\n",
    "Setting up `dbt` project from scratch often involves writing a lot of boilerplate from configuring the project to bringing in the sources and create staging models. While there are tools to semi-automate this process, there is still a lot of manual heavy-lifting that is required. In this notebook, I explore ways to automate this flow based on a highly opinionated way of organizing staging models. I will turn this into a Python package once I am settled on the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Project\n",
    "\n",
    "We start by initialize a dbt project. We will use a postgres adapter that will allow us to explore the project locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with dbt=0.21.0\n",
      "\n",
      "Your new dbt project \"dbt-foo\" was created! If this is your first time\n",
      "using dbt, you'll need to set up your profiles.yml file -- this file will tell dbt how\n",
      "to connect to your database. You can find this file by running:\n",
      "\n",
      "  xdg-open /home/gitpod/.dbt\n",
      "\n",
      "For more information on how to configure the profiles.yml file,\n",
      "please consult the dbt documentation here:\n",
      "\n",
      "  https://docs.getdbt.com/docs/configure-your-profile\n",
      "\n",
      "One more thing:\n",
      "\n",
      "Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:\n",
      "\n",
      "  https://community.getdbt.com/\n",
      "\n",
      "Happy modeling!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "dbt init dbt-foo --adapter postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update project name and profile name\n",
    "\n",
    "The default configuration file `dbt_project.yml` has a dummy project name (`my_new_project`) and profile (`default`). Let us update it based on the project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sed -i 's/my_new_project/dbt_foo/g' dbt-foo/dbt_project.yml \n",
    "sed -i 's/default/foo/g' dbt-foo/dbt_project.yml \n",
    "head -n -5 dbt-foo/dbt_project.yml > tmp.yml && mv tmp.yml dbt-foo/dbt_project.yml\n",
    "rm -rf models/example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with dbt=0.21.0\n",
      "dbt version: 0.21.0\n",
      "python version: 3.8.12\n",
      "python path: /home/gitpod/.pyenv/versions/3.8.12/bin/python3\n",
      "os info: Linux-5.13.0-1006-gcp-x86_64-with-glibc2.29\n",
      "Using profiles.yml file at /home/gitpod/.dbt/profiles.yml\n",
      "Using dbt_project.yml file at /workspace/dbt-explore/dbt-foo/dbt_project.yml\n",
      "\n",
      "Configuration:\n",
      "  profiles.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
      "  dbt_project.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
      "\n",
      "Required dependencies:\n",
      " - git [\u001b[32mOK found\u001b[0m]\n",
      "\n",
      "Connection:\n",
      "  host: localhost\n",
      "  port: 5432\n",
      "  user: corise\n",
      "  database: dbt\n",
      "  schema: dbt_toy_shop\n",
      "  search_path: None\n",
      "  keepalives_idle: 0\n",
      "  sslmode: None\n",
      "  Connection test: [\u001b[32mOK connection ok\u001b[0m]\n",
      "\n",
      "\u001b[32mAll checks passed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dbt debug --project-dir dbt-foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Sources\n",
    "\n",
    "The next step is to identify the sources to build the data models on top. A list of sources can be identified by listing the schemas under the database connection configured in `~/.dbt/profiles.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://corise:corise@localhost:5432/dbt\n",
    "%config SqlMagic.displaylimit=5\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       schema       \n",
      "--------------------\n",
      " pg_toast\n",
      " pg_temp_1\n",
      " pg_toast_temp_1\n",
      " pg_catalog\n",
      " public\n",
      " information_schema\n",
      "(6 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql -U postgres -c 'SELECT nspname AS schema FROM pg_catalog.pg_namespace;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on to the next step, let us import some useful python packages and write some handy utiility functions that will let us run `dbt` command line operations from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "@contextmanager\n",
    "def cwd(path: Path):\n",
    "    \"\"\"Sets the cwd within the context\n",
    "\n",
    "    Args:\n",
    "        path (Path): The path to the cwd\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    origin = Path().absolute()\n",
    "    try:\n",
    "        os.chdir(path)\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbt_run_operation(operation, **kwargs):\n",
    "    args_json = json.dumps(kwargs)\n",
    "    cmd = f\"dbt run-operation {operation} --args '{args_json}' | tail -n +2\"\n",
    "    out = subprocess.getoutput(cmd)\n",
    "    return(out)\n",
    "\n",
    "def write_as_yaml(x, file=None):\n",
    "    x_yaml = yaml.dump(x, sort_keys=False)\n",
    "    if file is None:\n",
    "      print(x_yaml)\n",
    "    else:\n",
    "      Path(file).write_text(x_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dbt-foo/packages.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dbt-foo/packages.yml\n",
    "packages:\n",
    "  - package: dbt-labs/codegen\n",
    "    version: 0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with dbt=0.21.0\n",
      "Installing dbt-labs/codegen@0.4.0\n",
      "  Installed from version 0.4.0\n",
      "  Up to date!\n",
      "Installing dbt-labs/dbt_utils@0.7.4\n",
      "  Installed from version 0.7.4\n",
      "  Up to date!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dbt deps --project-dir dbt-foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Source\n",
    "\n",
    "The next step to modeling in `dbt` is to identify the sources that need to be modelled. `dbt` has a command line tool that makes it easy to query a database schema and identify the tables in it. The `dbt_generate_source` function uses this tool to generate the source configuration based on a `database` and a `schema`. The `dbt_write_source` function writes a yaml file for the source config to `models/staging/<source_name>/<source_name>.yml`. This is a highly opinionated way of organizing the staging layer, and is based on the setup recommended by [dbt Labs](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source.yaml for foo to models/staging/foo/src_foo.yml\n"
     ]
    }
   ],
   "source": [
    "def dbt_generate_source(database, schema, name):\n",
    "    if name is None:\n",
    "        name = schema\n",
    "    source_yaml = dbt_run_operation('generate_source', database_name=database, schema_name=schema)\n",
    "    source_dict = yaml.safe_load(source_yaml)\n",
    "    return ({\n",
    "       \"version\": source_dict['version'],\n",
    "       \"sources\": [{\n",
    "           \"name\": name,\n",
    "           \"database\": database,\n",
    "           \"schema\": schema,\n",
    "           \"tables\": source_dict['sources'][0]['tables']\n",
    "       }]\n",
    "    })\n",
    "\n",
    "def dbt_write_source(source):\n",
    "  source_name = source['sources'][0]['name']\n",
    "  source_dir = Path(f\"models/staging/{source_name}\")\n",
    "  source_dir.mkdir(parents=True, exist_ok=True)\n",
    "  source_file = source_dir / f\"src_{source_name}.yml\"\n",
    "  print(f\"Writing source.yaml for {source_name} to {source_file}\")\n",
    "  write_as_yaml(source, source_file)\n",
    "\n",
    "with cwd('dbt-foo'):\n",
    "  source = dbt_generate_source('dbt', 'public', 'foo')\n",
    "  dbt_write_source(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Staging Models\n",
    "\n",
    "The next step is to bootstrap staging models for every source table. Once again `dbt` provides a really handy command line tool to generate the models and their configuration. The `dbt_generate_staging_models` function uses this tool to generate the boilerplate SQL for the staging model for every source table. The `dbt_write_staging_models` function writes these models to `models/staging/<source_name>/stg_<source_name>_<table_name>.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing staging models to models/staging/foo: 100%|██████████| 8/8 [00:00<00:00, 3566.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def dbt_generate_staging_models(source):\n",
    "    source_database = source['sources'][0]['database']\n",
    "    source_schema = source['sources'][0]['schema']\n",
    "    source_name = source['sources'][0]['name']\n",
    "    table_names = [table['name'] for table in source['sources'][0]['tables']]\n",
    "    staging_models = {\"name\": source_name, \"models\": {}}\n",
    "    pbar = tqdm(table_names)\n",
    "    for table_name in pbar:\n",
    "        pbar.set_description(f\"Generating staging model for {table_name}\")\n",
    "        sql = dbt_run_operation('generate_base_model', source_name = source_name, table_name = table_name)\n",
    "        staging_models['models'][table_name] = sql\n",
    "    return staging_models\n",
    "\n",
    "def dbt_write_staging_models(staging_models):\n",
    "    source_name = staging_models['name']\n",
    "    staging_model_dir = Path(f\"models/staging/{source_name}\")\n",
    "    staging_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    staging_model_items = tqdm(staging_models['models'].items())\n",
    "    staging_model_items.set_description(f\"Writing staging models to {staging_model_dir}\")\n",
    "    for staging_model_name, staging_model_sql in staging_model_items:\n",
    "        staging_model_file = staging_model_dir / f\"stg_{source_name}__{staging_model_name}.sql\"\n",
    "        staging_model_file.write_text(staging_model_sql)\n",
    "\n",
    "with cwd('dbt-foo'):\n",
    "    # staging_models = dbt_generate_staging_models(source)\n",
    "    dbt_write_staging_models(staging_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to think documentation first while building data models. Once again, `dbt` has a very useful utility to bootstrap the documentation for a single model. The `dbt_generate_staging_models_yaml` function uses this utility to loop through all staging models and returns a dictionary with the boilerplate documentation for all these models. The `dbt_write_staging_models_yaml` function then writes this to `models/staging/<source_name>/stg_<source_name>.yml`. It is important to run `dbt run` before running these two funtions, since otherwise, the column documentation is NOT generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with dbt=0.21.0\n",
      "Found 10 models, 4 tests, 0 snapshots, 0 analyses, 353 macros, 0 operations, 0 seed files, 8 sources, 0 exposures\n",
      "\n",
      "00:23:00 | Concurrency: 4 threads (target='dev')\n",
      "00:23:00 | \n",
      "00:23:00 | 1 of 10 START table model dbt_toy_shop.my_first_dbt_model............ [RUN]\n",
      "00:23:00 | 2 of 10 START view model dbt_toy_shop.stg_foo__addresses............. [RUN]\n",
      "00:23:00 | 3 of 10 START view model dbt_toy_shop.stg_foo__events................ [RUN]\n",
      "00:23:00 | 4 of 10 START view model dbt_toy_shop.stg_foo__order_items........... [RUN]\n",
      "00:23:00 | 2 of 10 OK created view model dbt_toy_shop.stg_foo__addresses........ [\u001b[32mCREATE VIEW\u001b[0m in 0.21s]\n",
      "00:23:00 | 1 of 10 OK created table model dbt_toy_shop.my_first_dbt_model....... [\u001b[32mSELECT 2\u001b[0m in 0.22s]\n",
      "00:23:00 | 3 of 10 OK created view model dbt_toy_shop.stg_foo__events........... [\u001b[32mCREATE VIEW\u001b[0m in 0.22s]\n",
      "00:23:00 | 5 of 10 START view model dbt_toy_shop.stg_foo__orders................ [RUN]\n",
      "00:23:00 | 6 of 10 START view model dbt_toy_shop.stg_foo__products.............. [RUN]\n",
      "00:23:00 | 4 of 10 OK created view model dbt_toy_shop.stg_foo__order_items...... [\u001b[32mCREATE VIEW\u001b[0m in 0.22s]\n",
      "00:23:00 | 7 of 10 START view model dbt_toy_shop.stg_foo__promos................ [RUN]\n",
      "00:23:00 | 8 of 10 START view model dbt_toy_shop.stg_foo__superheroes........... [RUN]\n",
      "00:23:00 | 6 of 10 OK created view model dbt_toy_shop.stg_foo__products......... [\u001b[32mCREATE VIEW\u001b[0m in 0.19s]\n",
      "00:23:00 | 9 of 10 START view model dbt_toy_shop.stg_foo__users................. [RUN]\n",
      "00:23:00 | 8 of 10 OK created view model dbt_toy_shop.stg_foo__superheroes...... [\u001b[32mCREATE VIEW\u001b[0m in 0.18s]\n",
      "00:23:00 | 5 of 10 OK created view model dbt_toy_shop.stg_foo__orders........... [\u001b[32mCREATE VIEW\u001b[0m in 0.20s]\n",
      "00:23:00 | 7 of 10 OK created view model dbt_toy_shop.stg_foo__promos........... [\u001b[32mCREATE VIEW\u001b[0m in 0.20s]\n",
      "00:23:00 | 10 of 10 START view model dbt_toy_shop.my_second_dbt_model........... [RUN]\n",
      "00:23:00 | 9 of 10 OK created view model dbt_toy_shop.stg_foo__users............ [\u001b[32mCREATE VIEW\u001b[0m in 0.08s]\n",
      "00:23:00 | 10 of 10 OK created view model dbt_toy_shop.my_second_dbt_model...... [\u001b[32mCREATE VIEW\u001b[0m in 0.07s]\n",
      "00:23:00 | \n",
      "00:23:00 | Finished running 9 view models, 1 table model in 0.66s.\n",
      "\n",
      "\u001b[32mCompleted successfully\u001b[0m\n",
      "\n",
      "Done. PASS=10 WARN=0 ERROR=0 SKIP=0 TOTAL=10\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!dbt run --project-dir dbt-foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing model yaml for users: 100%|██████████| 8/8 [00:34<00:00,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model yaml to models/staging/foo/stg_foo__addresses.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__events.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__order_items.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__orders.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__products.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__promos.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__superheroes.yml\n",
      "Writing model yaml to models/staging/foo/stg_foo__users.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def dbt_generate_staging_models_yaml(staging_models):\n",
    "    source_name = staging_models['name']\n",
    "    staging_models_yaml_dict = []\n",
    "    staging_model_names = tqdm(list(staging_models['models'].keys()))\n",
    "    for staging_model_name in staging_model_names:\n",
    "        staging_model_names.set_description(f\"Preparing model yaml for {staging_model_name}\")\n",
    "        staging_model_name = f\"stg_{source_name}__{staging_model_name}\"\n",
    "        # print(f\"Generating yaml for staging model {staging_model_name}\")\n",
    "        staging_model_yaml = dbt_run_operation('generate_model_yaml', model_name = staging_model_name)\n",
    "        staging_model_yaml_dict = yaml.safe_load(staging_model_yaml)\n",
    "        staging_models_yaml_dict = staging_models_yaml_dict + staging_model_yaml_dict['models']\n",
    "  \n",
    "    return {'name': source_name, 'models': staging_models_yaml_dict}\n",
    "\n",
    "def dbt_write_staging_models_yaml(staging_models_yaml):\n",
    "   source_name = staging_models_yaml['name']\n",
    "   staging_model_yaml_file = Path(f\"models/staging/{source_name}/stg_{source_name}.yml\")\n",
    "   out = {'version': 2, 'models': staging_models_yaml['models']}\n",
    "   print(f\"Writing model yaml to {staging_model_yaml_file}\")\n",
    "   write_as_yaml(out, staging_model_yaml_file)\n",
    "\n",
    "def dbt_write_staging_models_yaml_one_per_model(staging_models_yaml):\n",
    "    source_name = staging_models_yaml['name']\n",
    "    for staging_model in staging_models_yaml['models']:\n",
    "        model_name = staging_model['name']\n",
    "        staging_model_yaml_file = Path(f\"models/staging/{source_name}/{model_name}.yml\")\n",
    "        out = {'version': 2, 'models': [staging_model]}\n",
    "        print(f\"Writing model yaml to {staging_model_yaml_file}\")\n",
    "        write_as_yaml(out, staging_model_yaml_file)\n",
    "\n",
    "def dbt_write_staging_models_yaml_as_docstrings(staging_models_yaml):\n",
    "    source_name = staging_models_yaml['name']\n",
    "    for staging_model in staging_models_yaml['models']:\n",
    "        model_name = staging_model['name']\n",
    "        staging_model_file = Path(f\"models/staging/{source_name}/{model_name}.sql\")\n",
    "        staging_model_sql = staging_model_file.read_text()\n",
    "        staging_model_yaml = yaml.dump({\"columns\": staging_model[\"columns\"]}, sort_keys=False)\n",
    "        out = f\"/*\\n## Table {model_name}\\n\\n\\n```dbt \\n{ staging_model_yaml }```\\n*/\\n{ staging_model_sql }\"\n",
    "        staging_model_file.write_text(out)\n",
    "\n",
    "\n",
    "with cwd('dbt-foo'):\n",
    "    staging_models_yaml = dbt_generate_staging_models_yaml(staging_models)\n",
    "    dbt_write_staging_models_yaml_one_per_model(staging_models_yaml)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with dbt=0.21.0\n",
      "Found 10 models, 4 tests, 0 snapshots, 0 analyses, 353 macros, 0 operations, 0 seed files, 8 sources, 0 exposures\n",
      "\n",
      "00:23:53 | Concurrency: 4 threads (target='dev')\n",
      "00:23:53 | \n",
      "00:23:53 | 1 of 14 START table model dbt_toy_shop.my_first_dbt_model............ [RUN]\n",
      "00:23:53 | 2 of 14 START view model dbt_toy_shop.stg_foo__addresses............. [RUN]\n",
      "00:23:53 | 3 of 14 START view model dbt_toy_shop.stg_foo__events................ [RUN]\n",
      "00:23:53 | 4 of 14 START view model dbt_toy_shop.stg_foo__order_items........... [RUN]\n",
      "00:23:53 | 4 of 14 OK created view model dbt_toy_shop.stg_foo__order_items...... [\u001b[32mCREATE VIEW\u001b[0m in 0.21s]\n",
      "00:23:53 | 2 of 14 OK created view model dbt_toy_shop.stg_foo__addresses........ [\u001b[32mCREATE VIEW\u001b[0m in 0.22s]\n",
      "00:23:53 | 3 of 14 OK created view model dbt_toy_shop.stg_foo__events........... [\u001b[32mCREATE VIEW\u001b[0m in 0.22s]\n",
      "00:23:53 | 1 of 14 OK created table model dbt_toy_shop.my_first_dbt_model....... [\u001b[32mSELECT 2\u001b[0m in 0.22s]\n",
      "00:23:53 | 5 of 14 START view model dbt_toy_shop.stg_foo__orders................ [RUN]\n",
      "00:23:53 | 6 of 14 START view model dbt_toy_shop.stg_foo__products.............. [RUN]\n",
      "00:23:53 | 7 of 14 START view model dbt_toy_shop.stg_foo__promos................ [RUN]\n",
      "00:23:53 | 8 of 14 START view model dbt_toy_shop.stg_foo__superheroes........... [RUN]\n",
      "00:23:53 | 5 of 14 OK created view model dbt_toy_shop.stg_foo__orders........... [\u001b[32mCREATE VIEW\u001b[0m in 0.18s]\n",
      "00:23:53 | 7 of 14 OK created view model dbt_toy_shop.stg_foo__promos........... [\u001b[32mCREATE VIEW\u001b[0m in 0.18s]\n",
      "00:23:53 | 9 of 14 START view model dbt_toy_shop.stg_foo__users................. [RUN]\n",
      "00:23:53 | 10 of 14 START view model dbt_toy_shop.my_second_dbt_model........... [RUN]\n",
      "00:23:53 | 8 of 14 OK created view model dbt_toy_shop.stg_foo__superheroes...... [\u001b[32mCREATE VIEW\u001b[0m in 0.18s]\n",
      "00:23:53 | 6 of 14 OK created view model dbt_toy_shop.stg_foo__products......... [\u001b[32mCREATE VIEW\u001b[0m in 0.19s]\n",
      "00:23:53 | 11 of 14 START test not_null_my_first_dbt_model_id................... [RUN]\n",
      "00:23:53 | 12 of 14 START test unique_my_first_dbt_model_id..................... [RUN]\n",
      "00:23:54 | 11 of 14 FAIL 1 not_null_my_first_dbt_model_id....................... [\u001b[31mFAIL 1\u001b[0m in 0.14s]\n",
      "00:23:54 | 12 of 14 PASS unique_my_first_dbt_model_id........................... [\u001b[32mPASS\u001b[0m in 0.14s]\n",
      "00:23:54 | 9 of 14 OK created view model dbt_toy_shop.stg_foo__users............ [\u001b[32mCREATE VIEW\u001b[0m in 0.16s]\n",
      "00:23:54 | 10 of 14 OK created view model dbt_toy_shop.my_second_dbt_model...... [\u001b[32mCREATE VIEW\u001b[0m in 0.17s]\n",
      "00:23:54 | 13 of 14 START test not_null_my_second_dbt_model_id.................. [RUN]\n",
      "00:23:54 | 14 of 14 START test unique_my_second_dbt_model_id.................... [RUN]\n",
      "00:23:54 | 13 of 14 PASS not_null_my_second_dbt_model_id........................ [\u001b[32mPASS\u001b[0m in 0.04s]\n",
      "00:23:54 | 14 of 14 PASS unique_my_second_dbt_model_id.......................... [\u001b[32mPASS\u001b[0m in 0.04s]\n",
      "00:23:54 | \n",
      "00:23:54 | Finished running 9 view models, 1 table model, 4 tests in 0.79s.\n",
      "\n",
      "\u001b[31mCompleted with 1 error and 0 warnings:\u001b[0m\n",
      "\n",
      "\u001b[31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)\u001b[0m\n",
      "  Got 1 result, configured to fail if != 0\n",
      "\n",
      "  compiled SQL at target/compiled/dbt_foo/models/example/schema.yml/schema_test/not_null_my_first_dbt_model_id.sql\n",
      "\n",
      "Done. PASS=13 WARN=0 ERROR=1 SKIP=0 TOTAL=14\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dbt build --project-dir dbt-foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp dbt-greenery/target/catalog.json docs\n",
    "cp dbt-greenery/target/manifest.json docs\n",
    "cp dbt-greenery/target/run_results.json docs\n",
    "cp dbt-greenery/target/index.html docs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "625c31d6b4db3d7e7e2853cc30dc2062e1cda684f3e49d5f899ae496ae755fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
