{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Project\n",
    "\n",
    "The goal for the first week was to get set up with `dbt`, configure the project, create some models, add basic tests, and explore the dbt workflow to build models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![greenery-erd.png](greenery-erd.png)\n",
    "[https://dbdiagram.io/d/6199cada02cf5d186b6052df](https://dbdiagram.io/d/6199cada02cf5d186b6052df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Review Questions\n",
    "\n",
    "- ✅ Were you able to create schema.yml files with model names and descriptions? \n",
    "- ✅ Were you able to run your dbt models and snapshots against the data warehouse?\n",
    "- ✅ Could you run the queries to answer key questions from the project instructions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What was most challenging/surprising in completing this week’s project?__\n",
    "\n",
    "I was most challenged and surprised by five things while working on the project.\n",
    "\n",
    "1. __Boilerplate__: I was pleasantly surprised by the amount of boilerplate code one has to write to get set up. Importing the source tables and bootstrapping the staging models and the associated documentation was a largely manual effort. While dbt does provide some handy utilities for code generation, I still found the workflow to be largely manual. This motivated me to try my hand at automating this boilerplate and I have a [proof-of-concept](https://github.com/ramnathv/dbt-explore/blob/main/dbt-greenery/_automation.ipynb).\n",
    "\n",
    "2. __Conventions__: While `dbt` is opinionated in many ways, it is unopinionated in other ways, which was troublesome. I am a big believer in conventions over configuration, and so the ability to add configuration and properties in any `yaml` file in the project folder was confusing, leading to decision fatigue. However, I landed on this really handy [style guide](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md) from dbt Labs that gave me a solid set of conventions to follow, that I believe will scale as we expand the data model layers.\n",
    "\n",
    "3. __Tests__: `dbt` makes it really easy to translate logical checks on a model to simple configuration in a yaml file. An interesting byproduct of this simplicity is that it made me think about data quality a lot deeper and I ended up writing a lot more tests than I typically have the time for. This is great to track data quality at a much deeper level. I would love for the ability to dynamically populate a dashboard that has the test results along with profiling information, along the lines of what [Great Expectations](https://greatexpectations.io/) provides.\n",
    "\n",
    "4. __Snapshots__: This is a really cool concept and is a great way to be able to reproduce analyses going back in time. However, I was surprised by the amount of additional thought one needs to put in order to decide what tables to snapshot, and what strategy to use in order to snapshot them. I landed on the following framework to make decisions: (a) Are the rows mutable?, (b) Is there value in capturing history?, and (c) Is the table small? If the answer to all three questions is yes, it should be snapshotted. If the answer to (c) is No, then you trade off the benefit of (b) vs. the cost of (c), and decide accordingly. I would love to learn more about how others think about snapshotting in practice and what are some tips and tricks to handle them.\n",
    "\n",
    "5. __Workflow__: I am a big believer in repeatable workflows that can become routines that one can do spontaenously. After quite a bit of iteration, I landed on a reasonable worfklow: (1) tweak the staging model sql, (2) run the model to ensure it runs correctly, (3) add documentation and tests for each column, (4) rinse and repeat these steps. It was during this that I discovered the awesome `dbt build` command which basically runs `seed`, `snapshot`, `run`, and `test` in one go in that order to make it easier to iterate. I would love to learn more about other approaches to workflows and tips and tricks to make it efficient. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Is there a particular part of the project where you want focused feedback from your reviewers?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://corise:corise@localhost:5432/dbt\n",
    "%config SqlMagic.displaylimit=5\n",
    "%config SqlMagic.displaycon = False\n",
    "%config SqlMagic.feedback = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. How many users do we have?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>130</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(130,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(DISTINCT user_id) \n",
    "  FROM users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. On average, how many orders do we receive per hour?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>round</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>8.16</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(Decimal('8.16'),)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "WITH nb_orders_by_hour AS (\n",
    "SELECT DATE_TRUNC('hour', created_at) AS created_at_hour,\n",
    "       COUNT(DISTINCT order_id) AS nb_orders\n",
    "  FROM orders\n",
    " GROUP BY 1\n",
    ")\n",
    "\n",
    "SELECT ROUND(AVG(nb_orders), 2)\n",
    "  FROM nb_orders_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. On average, how long does an order take from being placed to being delivered?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>round</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3.93</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(Decimal('3.93'),)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "WITH nb_days_by_order AS (\n",
    "SELECT order_id, \n",
    "       created_at,\n",
    "       delivered_at,\n",
    "       EXTRACT(epoch FROM (delivered_at - created_at))/(24*3600) AS nb_days\n",
    "  FROM orders\n",
    " WHERE status = 'delivered'\n",
    ")\n",
    "\n",
    "SELECT ROUND(AVG(nb_days)::NUMERIC, 2)\n",
    "  FROM nb_days_by_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. How many users have only made one purchase? Two purchases? Three+ purchases?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>nb_purchases</th>\n",
       "        <th>nb_users</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3+</td>\n",
       "        <td>81</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('1', 25), ('2', 22), ('3+', 81)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "WITH nb_purchases_by_user AS (\n",
    "SELECT user_id,\n",
    "       COUNT(DISTINCT order_id) AS nb_purchases\n",
    "  FROM orders\n",
    " GROUP BY 1\n",
    ")\n",
    "\n",
    "SELECT CASE \n",
    "         WHEN nb_purchases < 3 THEN nb_purchases::VARCHAR \n",
    "         ELSE '3+'\n",
    "       END AS nb_purchases,\n",
    "       COUNT(user_id) AS nb_users\n",
    "  FROM nb_purchases_by_user\n",
    " GROUP BY 1\n",
    " ORDER BY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. On average, how many unique sessions do we have per hour?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>round</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>7.39</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(Decimal('7.39'),)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "WITH nb_sessions_by_hour AS (\n",
    "SELECT DATE_TRUNC('hour', created_at) AS created_at_hour,\n",
    "       COUNT(DISTINCT session_id) AS nb_sessions\n",
    "  FROM events\n",
    " GROUP BY 1\n",
    " ORDER BY 2 DESC\n",
    ")\n",
    "\n",
    "SELECT ROUND(AVG(nb_sessions)::NUMERIC, 2)\n",
    "  FROM nb_sessions_by_hour"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "625c31d6b4db3d7e7e2853cc30dc2062e1cda684f3e49d5f899ae496ae755fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
