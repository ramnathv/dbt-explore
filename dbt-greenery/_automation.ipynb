{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Boilerplate\n",
    "\n",
    "Setting up `dbt` project from scratch often involves writing a lot of boilerplate from configuring the project to bringing in the sources and create staging models. While there are tools to semi-automate this process, there is still a lot of manual heavy-lifting that is required. In this notebook, I explore ways to automate this flow based on a highly opinionated way of organizing staging models. I will turn this into a Python package once I am settled on the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Project\n",
    "\n",
    "```bash\n",
    "dbt init dbt-greenery --adapter postgres\n",
    "sed -i 's/my_project_name/dbt_greenery/g' dbt-greenery/dbt_project.yml \n",
    "sed -i 's/default/greenery/g' dbt-greenery/dbt_project.yml \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Sources\n",
    "\n",
    "The next step is to identify the sources to build the data models on top. A list of sources can be identified by listing the schemas under the database connection configured in `~/.dbt/profiles.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://corise:corise@localhost:5432/dbt\n",
    "%config SqlMagic.displaylimit=5\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       schema       \n",
      "--------------------\n",
      " pg_toast\n",
      " pg_temp_1\n",
      " pg_toast_temp_1\n",
      " pg_catalog\n",
      " public\n",
      " information_schema\n",
      "(6 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql -U postgres -c 'SELECT nspname AS schema FROM pg_catalog.pg_namespace;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on to the next step, let us import some useful python packages and write some handy utiility functions that will let us run `dbt` command line operations from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbt_run_operation(operation, **kwargs):\n",
    "    args_json = json.dumps(kwargs)\n",
    "    cmd = f\"dbt run-operation {operation} --args '{args_json}' | tail -n +2\"\n",
    "    out = subprocess.getoutput(cmd)\n",
    "    return(out)\n",
    "\n",
    "def write_as_yaml(x, file=None):\n",
    "    x_yaml = yaml.dump(x, sort_keys=False)\n",
    "    if file is None:\n",
    "      print(x_yaml)\n",
    "    else:\n",
    "      Path(file).write_text(x_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Source\n",
    "\n",
    "The next step to modeling in `dbt` is to identify the sources that need to be modelled. `dbt` has a command line tool that makes it easy to query a database schema and identify the tables in it. The `dbt_generate_source` function uses this tool to generate the source configuration based on a `database` and a `schema`. The `dbt_write_source` function writes a yaml file for the source config to `models/staging/<source_name>/<source_name>.yml`. This is a highly opinionated way of organizing the staging layer, and is based on the setup recommended by [dbt Labs](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbt_generate_source(database, schema, name):\n",
    "    if name is None:\n",
    "        name = schema\n",
    "    source_yaml = dbt_run_operation('generate_source', database_name=database, schema_name=schema)\n",
    "    source_dict = yaml.safe_load(source_yaml)\n",
    "    return ({\n",
    "       \"version\": source_dict['version'],\n",
    "       \"sources\": [{\n",
    "           \"name\": name,\n",
    "           \"database\": database,\n",
    "           \"schema\": schema,\n",
    "           \"tables\": source_dict['sources'][0]['tables']\n",
    "       }]\n",
    "    })\n",
    "\n",
    "def dbt_write_source(source):\n",
    "  source_name = source['sources'][0]['name']\n",
    "  source_dir = Path(f\"models/staging/{source_name}\")\n",
    "  source_dir.mkdir(parents=True, exist_ok=True)\n",
    "  source_file = source_dir / f\"src_{source_name}.yml\"\n",
    "  print(f\"Writing source yaml for {source_name} to {source_file}\")\n",
    "  write_as_yaml(source_greenery, f)\n",
    "\n",
    "source_greenery = dbt_generate_source('dbt', 'public', 'greenery')\n",
    "dbt_write_source(source_greenery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Staging Models\n",
    "\n",
    "The next step is to bootstrap staging models for every source table. Once again `dbt` provides a really handy command line tool to generate the models and their configuration. The `dbt_generate_staging_models` function uses this tool to generate the boilerplate SQL for the staging model for every source table. The `dbt_write_staging_models` function writes these models to `models/staging/<source_name>/stg_<source_name>_<table_name>.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbt_generate_staging_models(source):\n",
    "    source_database = source['sources'][0]['database']\n",
    "    source_schema = source['sources'][0]['schema']\n",
    "    source_name = source['sources'][0]['name']\n",
    "    table_names = [table['name'] for table in source['sources'][0]['tables']]\n",
    "    staging_models = {\"name\": source_name, \"models\": {}}\n",
    "    for table_name in table_names:\n",
    "        print(table_name)\n",
    "        sql = dbt_run_operation('generate_base_model', source_name = source_name, table_name = table_name)\n",
    "        staging_models['models'][table_name] = sql\n",
    "    return staging_models\n",
    "\n",
    "def dbt_write_staging_models(staging_models):\n",
    "    source_name = staging_models['name']\n",
    "    for staging_model_name, staging_model_sql in staging_models['models'].items():\n",
    "        staging_model_dir = Path(f\"models/staging/{source_name}\")\n",
    "        staging_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        staging_model_file = staging_model_dir / f\"stg_{source_name}__{staging_model_name}.sql\"\n",
    "        print(f\"Writing staging model for {staging_model_name} to {staging_model_file}\")\n",
    "        staging_model_file.write_text(staging_model_sql)\n",
    "\n",
    "staging_models_greenery = dbt_generate_staging_models(source_greenery)\n",
    "dbt_write_staging_models(staging_models_greenery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to think documentation first while building data models. Once again, `dbt` has a very useful utility to bootstrap the documentation for a single model. The `dbt_generate_staging_models_yaml` function uses this utility to loop through all staging models and returns a dictionary with the boilerplate documentation for all these models. The `dbt_write_staging_models_yaml` function then writes this to `models/staging/<source_name>/stg_<source_name>.yml`. It is important to run `dbt run` before running these two funtions, since otherwise, the column documentation is NOT generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating yaml for staging model stg_greenery__addresses\n",
      "Generating yaml for staging model stg_greenery__events\n",
      "Generating yaml for staging model stg_greenery__order_items\n",
      "Generating yaml for staging model stg_greenery__orders\n",
      "Generating yaml for staging model stg_greenery__products\n",
      "Generating yaml for staging model stg_greenery__promos\n",
      "Generating yaml for staging model stg_greenery__superheroes\n",
      "Generating yaml for staging model stg_greenery__users\n"
     ]
    }
   ],
   "source": [
    "def dbt_generate_staging_models_yaml(staging_models):\n",
    "    source_name = staging_models['name']\n",
    "    staging_models_yaml_dict = []\n",
    "    for staging_model_name in list(staging_models['models'].keys()):\n",
    "        staging_model_name = f\"stg_{source_name}__{staging_model_name}\"\n",
    "        print(f\"Generating yaml for staging model {staging_model_name}\")\n",
    "        staging_model_yaml = dbt_run_operation('generate_model_yaml', model_name = staging_model_name)\n",
    "        staging_model_yaml_dict = yaml.safe_load(staging_model_yaml)\n",
    "        staging_models_yaml_dict = staging_models_yaml_dict + staging_model_yaml_dict['models']\n",
    "  \n",
    "    return {'name': source_name, 'models': staging_models_yaml_dict}\n",
    "\n",
    "def dbt_write_staging_models_yaml(staging_models_yaml):\n",
    "   source_name = staging_models_yaml['name']\n",
    "   staging_model_yaml_file = Path(f\"models/staging/{source_name}/stg_{source_name}.yml\")\n",
    "   out = {'version': 2, 'models': staging_models_yaml['models']}\n",
    "   write_as_yaml(out, staging_model_yaml_file)\n",
    "\n",
    "staging_models_greenery_yaml = dbt_generate_staging_models_yaml(staging_models_greenery)\n",
    "dbt_write_staging_models_yaml(staging_models_greenery_yaml)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat target/manifest.json | jq '.nodes | to_entries | map({node: .key, materialized: .value.config.materialized})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "\n",
    "class DbtResourceType(str, Enum):\n",
    "    model = 'model'\n",
    "    analysis = 'analysis'\n",
    "    test = 'test'\n",
    "    operation = 'operation'\n",
    "    seed = 'seed'\n",
    "    source = 'source',\n",
    "    snapshot = 'snapshot'\n",
    "\n",
    "\n",
    "class DbtMaterializationType(str, Enum):\n",
    "    table = 'table'\n",
    "    view = 'view'\n",
    "    incremental = 'incremental'\n",
    "    ephemeral = 'ephemeral'\n",
    "    seed = 'seed',\n",
    "    snapshot = 'snapshot',\n",
    "    test = 'test'\n",
    "\n",
    "\n",
    "class NodeDeps(BaseModel):\n",
    "    nodes: List[str]\n",
    "\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    materialized: Optional[DbtMaterializationType]\n",
    "\n",
    "\n",
    "class Node(BaseModel):\n",
    "    unique_id: str\n",
    "    path: Path\n",
    "    resource_type: DbtResourceType\n",
    "    description: str\n",
    "    depends_on: Optional[NodeDeps]\n",
    "    config: NodeConfig\n",
    "\n",
    "\n",
    "class Manifest(BaseModel):\n",
    "    nodes: Dict[\"str\", Node]\n",
    "    sources: Dict[\"str\", Node]\n",
    "\n",
    "    @validator('nodes', 'sources')\n",
    "    def filter(cls, val):\n",
    "        return {k: v for k, v in val.items() if v.resource_type.value in ('model', 'seed', 'source')}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"target/manifest.json\") as fh:\n",
    "        data = json.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nodes']['model.dbt_greenery.dim_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat target/manifest.json | \\\n",
    "    jq '.nodes | to_entries | map({node: .key, compiled_sql: .value.compiled_sql, dependencies: .value.depends_on.nodes})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "class GraphManifest(Manifest):\n",
    "    @property\n",
    "    def node_list(self):\n",
    "        return list(self.nodes.keys()) + list(self.sources.keys())\n",
    "\n",
    "    @property\n",
    "    def edge_list(self):\n",
    "        return [(k, d) for k, v in self.nodes.items() for d in v.depends_on.nodes]\n",
    "\n",
    "    def build_graph(self) -> nx.Graph:\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.node_list)\n",
    "        G.add_edges_from(self.edge_list)\n",
    "        return G\n",
    "m = GraphManifest(**data)\n",
    "G = m.build_graph()\n",
    "nx.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dbt.fct_register_event()\n",
    "       .merge(dbt.dim_event(), how='left', on='event_id',  suffixes=('', '_y'))\n",
    "       [['session_id', 'event_created_at', 'event_type']]\n",
    "       .to_csv('events.csv', index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fct_register_event = pd.read_sql('SELECT * FROM dbt_ramnath_v.fct_register_event', con)\n",
    "dim_event = pd.read_sql('SELECT * FROM dbt_ramnath_v.dim_event', con)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "625c31d6b4db3d7e7e2853cc30dc2062e1cda684f3e49d5f899ae496ae755fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
